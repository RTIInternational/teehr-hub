{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9d6d94-7b0c-4027-a6aa-69267f61f59a",
   "metadata": {},
   "source": [
    "#### This notebook calculates mean areal precip (MAP) for the 40-year NWM v3.0 retrospective for USGS high-res basin polygons (CONUS only)\n",
    "- Requires the weights (fractional pixel coverage) to be pre-calculated (see `05_calculate_nwm30_pixel_weights.ipynb`)\n",
    "- Reads NWM v3.0 forcing (RAINRATE) netcdf files from s3 using Sedona: https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/index.html#CONUS/\n",
    "- Writes output to local TEEHR Evaluation (warehouse)\n",
    "- Some additional maintenance and exploratory code is included below\n",
    "- PROCESSING NOTES:\n",
    "    - Still working on defining the optimal balance between number of cores, executors, and memory.\n",
    "    - Still seeing occasional pods die with OOM errors.\n",
    "    - Processing chunks of timesteps in a loop may be resulting in memory build up, although the `del` and garbage collecting seems to have helped.\n",
    "    - Definitely more to understand here in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9eb210-72cc-4b42-99e5-72e615b33df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import teehr\n",
    "from teehr.evaluation.spark_session_utils import create_spark_session\n",
    "\n",
    "LINE_PLOT_HEIGHT = 300\n",
    "LINE_PLOT_WIDTH = 600\n",
    "\n",
    "# Set global defaults for all line plots\n",
    "hv.opts.defaults(\n",
    "    hv.opts.Curve(\n",
    "        bgcolor=\"#e7e9ecb8\",\n",
    "        show_grid=True,\n",
    "        gridstyle={'grid_line_alpha': 0.5, 'grid_line_color': 'white'},\n",
    "        frame_width=LINE_PLOT_WIDTH,\n",
    "        frame_height=LINE_PLOT_HEIGHT\n",
    "    )\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "teehr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28f315-85a0-40fd-b8e9-2019867dcc85",
   "metadata": {},
   "source": [
    "Configure the logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610556b-1341-4b99-9efb-323f8d8af16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "# Create a file handler, set its level, and define its format.\n",
    "file_handler = logging.FileHandler('sedona_map_processor_logger.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "# Add the file handler to the logger.\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40f04d-d873-41ba-a487-e19a1c6370a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44947135-413b-4bcd-bf44-d6cf29fc3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # ~7 pods/node --> Resulted in OOMKilled msgs in the driver pod logs (140 / 7 = 20 nodes)\n",
    "# NUM_EXECUTORS = 140\n",
    "# NUM_CORES = 2\n",
    "# EXECUTOR_MEMORY = \"15g\"\n",
    "\n",
    "# # ~4 pods/node\n",
    "# NUM_EXECUTORS = 80\n",
    "# NUM_CORES = 3\n",
    "# EXECUTOR_MEMORY = \"26g\"\n",
    "\n",
    "# ~3 pods/node\n",
    "NUM_EXECUTORS = 60\n",
    "NUM_CORES = 4\n",
    "EXECUTOR_MEMORY = \"35g\"\n",
    "\n",
    "NUM_SHUFFLE_PARTITIONS = NUM_EXECUTORS * NUM_CORES * 2\n",
    "\n",
    "spark = create_spark_session(\n",
    "    start_spark_cluster=True,\n",
    "    executor_instances=NUM_EXECUTORS,\n",
    "    executor_memory=EXECUTOR_MEMORY,\n",
    "    executor_cores=NUM_CORES,\n",
    "    aws_region=\"us-east-1\",\n",
    "    update_configs={\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\":\n",
    "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "        \"spark.sql.shuffle.partitions\": f\"{NUM_SHUFFLE_PARTITIONS}\",\n",
    "        \"spark.kubernetes.executor.node.selector.teehr-hub/nodegroup-name\": \"spark-r5-4xlarge-spot\",\n",
    "        \"spark.decommission.enabled\": \"true\",\n",
    "        \"spark.executor.decommission.signal\": \"SIGTERM\",\n",
    "        \"spark.storage.decommission.enabled\": \"true\",\n",
    "        # \"spark.storage.decommission.rddBlocks.enabled\": \"true\",  # default is true\n",
    "        # \"spark.storage.decommission.shuffleBlocks.enabled\": \"true\",  # default is true\n",
    "        # \"spark.storage.decommission.fallbackStorage.path\": \"s3a://ciroh-rti-public-data/spark-fallback-storage/\",\n",
    "        # \"spark.kubernetes.driver.ownPersistentVolumeClaim\": \"true\",\n",
    "        # \"spark.kubernetes.driver.reusePersistentVolumeClaim\": \"true\"\n",
    "    }    \n",
    ")\n",
    "\n",
    "dir_path = \"/data/playground/slamont/teehr/warehouse/sedona/usgs_basins_map\"\n",
    "\n",
    "# USE EXISTING:\n",
    "ev = teehr.Evaluation(\n",
    "    spark=spark,\n",
    "    dir_path=dir_path,\n",
    "    create_dir=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912aec5-4bbb-46c0-ab3d-aab16315b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_PATTERN = r\"(\\d{12})\"\n",
    "CONFIGURATION_NAME = \"nwm30_retrospective\"\n",
    "UNIT_NAME = \"mm/s\"\n",
    "VARIABLE_NAME = \"rainfall_hourly_rate\"\n",
    "\n",
    "# Create filepath generator\n",
    "BASE_S3_PATH = \"s3a://noaa-nwm-retrospective-3-0-pds/CONUS/netcdf/FORCING/\"\n",
    "\n",
    "START_DATE = \"2013-05-24 17:00:00\"\n",
    "END_DATE = \"2023-01-31 23:00\"   # Last file: 202301312300.LDASIN_DOMAIN1\n",
    "\n",
    "CHUNK_SIZE = 210  # Number of timesteps (hours) processed at once (1 year test used 200, but can go higher)  INCREASE for more efficiency/less IO but more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e9d1a-34d0-4c3f-827a-fae65d3476f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp(START_DATE)\n",
    "end_date = pd.Timestamp(END_DATE)\n",
    "dt_rng = pd.date_range(start=START_DATE, end=END_DATE, freq=\"h\")\n",
    "\n",
    "NUM_SPLITS = int(len(dt_rng) / CHUNK_SIZE)\n",
    "full_filepaths = [f\"{BASE_S3_PATH}{dt.year}/{dt.year}{dt.month:02d}{dt.day:02d}{dt.hour:02d}00.LDASIN_DOMAIN1\" for dt in dt_rng]\n",
    "\n",
    "split_full_filepaths = np.array_split(full_filepaths, NUM_SPLITS)\n",
    "\n",
    "len(split_full_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200268a8-96a1-4456-ad76-9650313f965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a view of fractional coverage table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW fractions_view AS\n",
    "    SELECT fraction_covered, id AS location_id, pos AS position_index FROM local.teehr.nwm30_usgs_hires_basins_fractional_coverage\n",
    "\"\"\")\n",
    "spark.sql(\"CACHE TABLE fractions_view\")\n",
    "\n",
    "# Try this for lower memory requirements but higher cpu requirements:\n",
    "# fractions_df = spark.table(\"fractions_view\")\n",
    "# fractions_df.persist(StorageLevel.MEMORY_AND_DISK_2)  # <-- Adding 2 replicates data across 2 nodes for fault tolerance\n",
    "# fractions_df.count()  # Materialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4915d-4a2f-48e8-826c-8ef514efa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "table_name = \"primary_timeseries\"\n",
    "\n",
    "cntr = 0\n",
    "for i, split_filepaths_chunk in enumerate(split_full_filepaths):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    filepaths = [str(fp) for fp in split_filepaths_chunk]\n",
    "\n",
    "    nc_sdf = spark.read.format(\"binaryFile\").load(filepaths).selectExpr(\"RS_FromNetCDF(content, 'RAINRATE', 'x', 'y') as raster\", \"path as filepath\") \n",
    "    nc_sdf = nc_sdf.withColumn(\"value_time\", F.regexp_extract(nc_sdf[\"filepath\"], DATE_PATTERN, 1))  # partition by time?\n",
    "    \n",
    "    # Explode the raster values\n",
    "    raster_exp_sdf = nc_sdf.selectExpr(\n",
    "        \"posexplode(RS_BandAsArray(raster, 1))\",\n",
    "        \"value_time\",\n",
    "    ).selectExpr(\n",
    "        \"value_time as value_time\",\n",
    "        \"col as value\",\n",
    "        \"CAST(pos as BIGINT) as position_index\"\n",
    "    )\n",
    "    raster_exp_sdf.createOrReplaceTempView(\"raster_values\")\n",
    "    \n",
    "    # Calculate MAP\n",
    "    map_results = spark.sql(f\"\"\"\n",
    "        SELECT /*+ BROADCAST(w) */\n",
    "            w.location_id,\n",
    "            to_timestamp(r.value_time, 'yyyyMMddHHmm') AS value_time,\n",
    "            SUM(r.value * w.fraction_covered) / SUM(w.fraction_covered) AS value,\n",
    "            CAST(NULL AS TIMESTAMP) AS reference_time,\n",
    "            '{UNIT_NAME}' AS unit_name,\n",
    "            '{VARIABLE_NAME}' AS variable_name,\n",
    "            '{CONFIGURATION_NAME}' AS configuration_name\n",
    "        FROM \n",
    "            raster_values AS r\n",
    "        JOIN \n",
    "             fractions_view AS w ON r.position_index = w.position_index\n",
    "        GROUP BY \n",
    "            w.location_id, r.value_time;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Write to table\n",
    "    ev.write.to_warehouse(\n",
    "        source_data=map_results,\n",
    "        table_name=table_name,  # Note. 1-year run stored in \"temp_secondary_timeseries\" table\n",
    "        write_mode=\"append\"\n",
    "    )\n",
    "\n",
    "    spark.catalog.dropTempView(\"raster_values\")\n",
    "\n",
    "    del raster_exp_sdf\n",
    "    gc.collect()\n",
    "    logger.info(f\"Processed chunk {i}/{len(split_full_filepaths)} in {(time.time() - t0) / 60:.2f} mins\")\n",
    "\n",
    "\n",
    "    # # Rewrite data files to fix the small file problem every 100 days or so\n",
    "    # cntr += 1\n",
    "    # days_processed = (CHUNK_SIZE * cntr) / 24\n",
    "    # if days_processed > 200:\n",
    "    #     t1 = time.time()\n",
    "    #     ev.spark.sql(f\"\"\"\n",
    "    #         CALL local.system.rewrite_data_files(\n",
    "    #             table => 'teehr.{table_name}',\n",
    "    #             options => map('target-file-size-bytes', '134217728') -- 128 MB\n",
    "    #         )\n",
    "    #     \"\"\")\n",
    "    #     cntr = 0\n",
    "    #     print(f\"Rewrote data files in {(time.time() - t1) / 60:.2f} mins\")\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07f623-e915-4faa-9a63-567aa561adc4",
   "metadata": {},
   "source": [
    "### One time requirement: Let's load the first year we've calculated locally into the warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20826a-5f2e-448e-916d-13fa73bda71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/data/playground/slamont/teehr/warehouse/sedona/usgs_basins_map\"\n",
    "\n",
    "spark = create_spark_session()\n",
    "\n",
    "# USE EXISTING:\n",
    "ev = teehr.Evaluation(\n",
    "    spark=spark,\n",
    "    dir_path=dir_path,\n",
    "    create_dir=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61374aa-fc04-47b3-a618-a032ade2a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ev.table(table_name=\"temp_secondary_timeseries\").to_sdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b55d1e-d8c8-46a0-9958-1886a012d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2d809-e1f1-4a55-af2c-b78d00599b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_sdf = sdf.drop(\"member\")\n",
    "primary_sdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45db85c-d2f5-40f0-8d57-4b32a48d39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_sdf.select(F.max(\"value_time\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf9861-f17d-4480-9006-ed36f92eb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_times = [row['value_time'] for row in primary_sdf.select('value_time').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ecde9-eb3f-4374-a877-3ba74ee27085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_sdf = primary_sdf.withColumn(\"reference_time\", F.to_timestamp(F.col(\"reference_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "# primary_sdf = primary_sdf.withColumn(\"value_time\", F.to_timestamp(F.col(\"value_time\"), \"yyyyMMddHHmm\"))\n",
    "# primary_sdf.show(4)\n",
    "\n",
    "primary_casted_sdf = primary_sdf.withColumn(\"value_time\", F.to_timestamp(F.col(\"value_time\"), \"yyyyMMddHHmm\")).withColumn(\"reference_time\", F.to_timestamp(F.col(\"reference_time\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391610f0-cf39-4651-8565-c0af5cdd309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_casted_sdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceecb82-83a0-4fdd-9a5d-b92a19dc514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.set_active_catalog(\"remote\")\n",
    "ev.active_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713d76f-8a0d-4069-bb3e-80107653ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.write.to_warehouse(\n",
    "    source_data=primary_casted_sdf,\n",
    "    table_name=\"primary_timeseries\",\n",
    "    write_mode=\"append\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb61f4-86cf-4e20-a843-6c1660ac391a",
   "metadata": {},
   "source": [
    "### Alter table partitions and rewrite datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c80e5-34e3-4a3b-99df-607f5865ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.spark.sql(\"ALTER TABLE local.teehr.primary_timeseries ADD PARTITION FIELD years(value_time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead45dab-bc88-488d-94ce-269f25c9d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.spark.sql(\"ALTER TABLE local.teehr.primary_timeseries ADD PARTITION FIELD months(value_time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb508402-c139-4b86-b750-4de3373e8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set a target file size (e.g., 128 MB)\n",
    "table_name = \"primary_timeseries\"\n",
    "\n",
    "ev.spark.sql(f\"\"\"\n",
    "    CALL local.system.rewrite_data_files(\n",
    "        table => 'teehr.{table_name}',\n",
    "        options => map('target-file-size-bytes', '134217728') -- 128 MB\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c2d53-8ab0-4b05-a1e2-ee21c33675c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.spark.sql(\"SELECT committed_at FROM local.teehr.primary_timeseries.snapshots ORDER BY committed_at DESC\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661f408-2220-4b78-bdb4-6fbd834e5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ev.spark.sql(\"\"\"\n",
    "    CALL local.system.expire_snapshots(\n",
    "        table => 'teehr.primary_timeseries', \n",
    "        older_than => TIMESTAMP '2026-02-11 01:14:22.558',\n",
    "        retain_last => 1\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72c065-3913-4a50-a7c1-3cdd9a12d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Coalesce approach\n",
    "# table_data_dir = \"/data/playground/slamont/teehr/warehouse/sedona/usgs_basins_map/local/teehr/primary_timeseries/data\"\n",
    "\n",
    "# sdf = ev.spark.read.parquet(str(table_data_dir / \"*.parquet\"))\n",
    "# sdf.coalesce(num_cache_files).write.mode(\"overwrite\").parquet(str(coalesced_cache_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c35097-694b-455d-8fc3-c2ef696e765d",
   "metadata": {},
   "source": [
    "## Exploring the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fa663-acee-4204-9309-fd832e69228d",
   "metadata": {},
   "source": [
    "#### I needed to re-register the tables after we switch to the JDBC local catalog instead of hadooop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ab76f-15c1-48ab-b8f8-9ae2a41b5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\n",
    "    \"attributes\",\n",
    "    \"configurations\",\n",
    "    \"grid_pixel_coverage_weights\",\n",
    "    \"location_attributes\",\n",
    "    \"location_crosswalks\",\n",
    "    \"locations\",\n",
    "    \"nwm30_usgs_hires_basins_fractional_coverage\",\n",
    "    \"primary_timeseries\",\n",
    "    \"secondary_timeseries\",\n",
    "    \"temp_nwm_rainrate_rasters\",\n",
    "    \"temp_secondary_timeseries\",\n",
    "    \"temp_secondary_timeseries_test\",\n",
    "    \"units\",\n",
    "    \"variables\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11fb3f-218a-41fa-adfa-c3ff287f03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the register_table procedure\n",
    "for table_name in table_names:\n",
    "    meta_dir = f\"/data/playground/slamont/teehr/warehouse/sedona/usgs_basins_map/local/teehr/{table_name}/metadata\"\n",
    "    filelist = glob.glob(meta_dir + \"/*.metadata.json\")\n",
    "    meta_df = pd.DataFrame([{\"path\": fullpath, \"version_number\": int(re.findall(r'v(\\d+)', Path(fullpath).stem)[0])} for fullpath in filelist])\n",
    "    latest_meta_path = meta_df.iloc[meta_df.version_number.idxmax()].path\n",
    "\n",
    "    # NOTE: If it already exists this will raise an error\n",
    "    ev.spark.sql(f\"\"\"\n",
    "    CALL local.system.register_table(\n",
    "        table => 'teehr.{table_name}',\n",
    "        metadata_file => '{latest_meta_path}'\n",
    "    )\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44bbf0-6ccf-4c61-b043-5d1a7b03f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev.list_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cf909-effd-4549-919d-ae95ae2b72e1",
   "metadata": {},
   "source": [
    "#### Check out the start/end times, total timesteps, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a1b66-27a2-4a3f-9aae-cecf9c62c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ev.primary_timeseries.to_sdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ce445-8433-44a5-b53c-b5674f6add76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf.select(F.max(\"value_time\")).show(), sdf.select(F.min(\"value_time\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd659bd2-9980-4d86-9893-d5b6afd32bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hour_sdf = sdf.filter(\"value_time = '1984-01-01 06:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb2a87-1ccb-46ae-b8cf-594e5b292f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sed_df = one_hour_sdf.toPandas()\n",
    "sed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96df0de-4007-49b3-a5e1-647fbaa7f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_df = pd.read_csv(\"/data/playground/slamont/teehr/warehouse/sedona/exactextract/198401010600_RAINRATE_nwm_v30_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e302d-564c-4a7b-b3f1-b9f529d58359",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_df.set_index(\"basin_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af1239-24eb-4cc2-8998-ec2adc954cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dda862-9bfd-43e5-af55-54a49e568874",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_df[ee_df.basin_id == \"usgsbasin-01013500\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ca65a-dbab-40bf-9dc7-95a5b4a16e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_df.sort_values(by=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ff46f-7e96-4ea5-b408-705ee57e0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sed_df.sort_values(by=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed43c1e-5788-4dd3-bf40-83c00ffe9157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
