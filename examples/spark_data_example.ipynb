{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1adc0e",
   "metadata": {},
   "source": [
    "# Simple Spark Data Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Creating a Spark session with executors\n",
    "2. Creating sample data\n",
    "3. Writing data to the `/data` shared volume\n",
    "4. Reading the data back\n",
    "5. Performing basic transformations\n",
    "\n",
    "This tests both the Spark connectivity and the shared storage mount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19653fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Add the utils directory to the path\n",
    "sys.path.append('/opt/teehr')\n",
    "\n",
    "from simple_spark_helper import create_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b740803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test write access to MinIO to in-cluster bucket.\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ada0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with 2 executors for testing\n",
    "spark = create_spark_session(\n",
    "    app_name=\"TEEHR-Data-Example\",\n",
    "    executor_instances=2,\n",
    "    executor_memory=\"1g\",\n",
    "    executor_cores=1,\n",
    "    driver_memory=\"1g\"\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Spark session ready!\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba927e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic Spark functionality\n",
    "print(\"üìä Testing basic Spark operations...\")\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(i, f\"name_{i}\", random.uniform(10.0, 100.0)) for i in range(1000)]\n",
    "columns = [\"id\", \"name\", \"value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(f\"‚úÖ Created DataFrame with {df.count()} rows\")\n",
    "print(\"\\nüìã Sample data:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that /data directory is accessible\n",
    "data_dir = \"/data\"\n",
    "print(f\"üìÅ Checking data directory: {data_dir}\")\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"‚úÖ Data directory exists\")\n",
    "    print(f\"   Directory contents: {os.listdir(data_dir)}\")\n",
    "    \n",
    "    # Test write permissions\n",
    "    test_file = os.path.join(data_dir, \"test_write.txt\")\n",
    "    try:\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        print(f\"‚úÖ Write permissions confirmed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Write permission test failed: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Data directory does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check detailed permissions and ownership\n",
    "import subprocess\n",
    "import stat\n",
    "\n",
    "print(f\"üîç Detailed investigation of {data_dir}:\")\n",
    "\n",
    "# Check permissions and ownership\n",
    "try:\n",
    "    stat_info = os.stat(data_dir)\n",
    "    mode = stat.filemode(stat_info.st_mode)\n",
    "    uid = stat_info.st_uid\n",
    "    gid = stat_info.st_gid\n",
    "    \n",
    "    print(f\"   Permissions: {mode}\")\n",
    "    print(f\"   Owner UID: {uid}\")\n",
    "    print(f\"   Group GID: {gid}\")\n",
    "    \n",
    "    # Check current user\n",
    "    import pwd\n",
    "    import os\n",
    "    current_uid = os.getuid()\n",
    "    current_gid = os.getgid()\n",
    "    \n",
    "    try:\n",
    "        user_info = pwd.getpwuid(current_uid)\n",
    "        username = user_info.pw_name\n",
    "    except:\n",
    "        username = \"unknown\"\n",
    "    \n",
    "    print(f\"   Current user: {username} (UID: {current_uid}, GID: {current_gid})\")\n",
    "    \n",
    "    # Check if we can read the directory\n",
    "    readable = os.access(data_dir, os.R_OK)\n",
    "    writable = os.access(data_dir, os.W_OK)\n",
    "    executable = os.access(data_dir, os.X_OK)\n",
    "    \n",
    "    print(f\"   Directory access: Read={readable}, Write={writable}, Execute={executable}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Error getting stat info: {e}\")\n",
    "\n",
    "# Try to see what's inside with ls -la\n",
    "try:\n",
    "    result = subprocess.run(['ls', '-la', data_dir], capture_output=True, text=True)\n",
    "    print(f\"\\nüìÇ Directory listing (ls -la {data_dir}):\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(f\"   Errors: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Could not run ls command: {e}\")\n",
    "\n",
    "# Check parent directory permissions too\n",
    "parent_dir = os.path.dirname(data_dir.rstrip('/'))\n",
    "if parent_dir and parent_dir != data_dir:\n",
    "    try:\n",
    "        result = subprocess.run(['ls', '-la', parent_dir], capture_output=True, text=True)\n",
    "        print(f\"\\nüìÅ Parent directory listing (ls -la {parent_dir}):\")\n",
    "        print(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not list parent directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc952a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to the /data directory as Parquet\n",
    "output_path = \"/data/spark_example_data\"\n",
    "\n",
    "print(f\"üíæ Writing data to: {output_path}\")\n",
    "\n",
    "# Write as Parquet with overwrite mode\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .parquet(output_path)\n",
    "\n",
    "print(\"‚úÖ Data written successfully!\")\n",
    "\n",
    "# Check what was created\n",
    "if os.path.exists(output_path):\n",
    "    files = os.listdir(output_path)\n",
    "    print(f\"üìÇ Files created: {files}\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    for file in files:\n",
    "        file_path = os.path.join(output_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"   üìÑ {file}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back from /data\n",
    "print(f\"üìñ Reading data back from: {output_path}\")\n",
    "\n",
    "df_read = spark.read.parquet(output_path)\n",
    "\n",
    "print(f\"‚úÖ Successfully read {df_read.count()} rows\")\n",
    "print(\"\\nüîç Schema of read data:\")\n",
    "df_read.printSchema()\n",
    "\n",
    "print(\"\\nüìã Sample of read data:\")\n",
    "df_read.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some basic transformations to test Spark processing\n",
    "print(\"üîÑ Performing data transformations...\")\n",
    "\n",
    "# Add some computed columns\n",
    "from pyspark.sql.functions import col, when, round, avg, max, min, count\n",
    "\n",
    "df_transformed = df_read.withColumn(\n",
    "    \"value_rounded\", round(col(\"value\"), 2)\n",
    ").withColumn(\n",
    "    \"category\", when(col(\"value\") < 30, \"low\")\n",
    "                .when(col(\"value\") < 70, \"medium\")\n",
    "                .otherwise(\"high\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Added computed columns\")\n",
    "df_transformed.show(10)\n",
    "\n",
    "# Perform aggregations\n",
    "print(\"\\nüìä Computing aggregations...\")\n",
    "stats = df_transformed.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"value\").alias(\"avg_value\"),\n",
    "    min(\"value\").alias(\"min_value\"),\n",
    "    max(\"value\").alias(\"max_value\")\n",
    ")\n",
    "\n",
    "print(\"üìà Statistics by category:\")\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple write formats to /data\n",
    "print(\"üìù Testing different file formats...\")\n",
    "\n",
    "# Take a smaller sample for format testing\n",
    "sample_df = df_transformed.limit(100)\n",
    "\n",
    "formats_to_test = {\n",
    "    \"csv\": \"/data/spark_example.csv\",\n",
    "    \"json\": \"/data/spark_example.json\",\n",
    "    \"parquet\": \"/data/spark_example.parquet\"\n",
    "}\n",
    "\n",
    "for format_name, path in formats_to_test.items():\n",
    "    try:\n",
    "        if format_name == \"csv\":\n",
    "            sample_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(path)\n",
    "        elif format_name == \"json\":\n",
    "            sample_df.coalesce(1).write.mode(\"overwrite\").json(path)\n",
    "        elif format_name == \"parquet\":\n",
    "            sample_df.write.mode(\"overwrite\").parquet(path)\n",
    "        \n",
    "        print(f\"‚úÖ {format_name.upper()} written to {path}\")\n",
    "        \n",
    "        # Check file size\n",
    "        if os.path.exists(path):\n",
    "            total_size = sum(os.path.getsize(os.path.join(path, f)) \n",
    "                           for f in os.listdir(path) \n",
    "                           if os.path.isfile(os.path.join(path, f)))\n",
    "            print(f\"   üìä Total size: {total_size:,} bytes\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to write {format_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32065c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and show summary\n",
    "print(\"\\nüßπ Cleaning up and summary...\")\n",
    "\n",
    "# Show final directory contents\n",
    "print(\"\\nüìÇ Final /data directory contents:\")\n",
    "for item in os.listdir(\"/data\"):\n",
    "    item_path = os.path.join(\"/data\", item)\n",
    "    if os.path.isdir(item_path):\n",
    "        file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
    "        print(f\"   üìÅ {item}/ ({file_count} files)\")\n",
    "    else:\n",
    "        size = os.path.getsize(item_path)\n",
    "        print(f\"   üìÑ {item} ({size:,} bytes)\")\n",
    "\n",
    "# Show Spark application info\n",
    "print(f\"\\nüéØ Spark Application Summary:\")\n",
    "print(f\"   - Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   - Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   - Master: {spark.sparkContext.master}\")\n",
    "print(f\"   - Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "# Note: Executor count not available in this Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9594e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE iceberg;\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS teehr;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170accf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS iceberg.teehr.sample\")\n",
    "sample_df.writeTo(\"iceberg.teehr.sample\").create()\n",
    "spark.sql(\"SELECT * FROM iceberg.teehr.sample\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "print(\"üõë Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped successfully!\")\n",
    "\n",
    "print(\"\\nüéâ Example completed successfully!\")\n",
    "print(\"\\nüìù What we tested:\")\n",
    "print(\"   ‚úÖ Spark session creation with Kubernetes executors\")\n",
    "print(\"   ‚úÖ Data creation and basic transformations\")\n",
    "print(\"   ‚úÖ Writing data to shared /data volume\")\n",
    "print(\"   ‚úÖ Reading data back from /data volume\")\n",
    "print(\"   ‚úÖ Multiple file formats (CSV, JSON, Parquet)\")\n",
    "print(\"   ‚úÖ Aggregations and distributed processing\")\n",
    "print(\"   ‚úÖ Persistent storage verification\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
